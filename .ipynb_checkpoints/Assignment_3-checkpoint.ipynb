{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Joseph Krinke*\n",
    "Netid: jdk61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "This assignment will provide structured practice to help enable you to...\n",
    "1. Understand the primary workflow in machine learning: (1) identifying a hypothesis function set of models, (2) determining a loss/cost/error/objective function to minimize, and (3) minimizing that function through gradient descent\n",
    "2. Implement batch gradient descent and become familiar with how that technique is used and its dependence on the choice of learning rate\n",
    "3. Gain practice in implementing machine learning algorithms to understand the math and programming behind them to achieve practical proficiency with the techniques\n",
    "4. Evaluate supervised learning algorithm performance through ROC curves and using cross validation\n",
    "5. Work with imagery data and the basics of computer vision approaches to machine learning\n",
    "6. Develop an understanding the optimal minimum misclassification error classifier (Bayes' classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAC USERS TAKE NOTE:\n",
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "**(a)** What is the likelihood function for all the $N$ samples in our training dataset that we will wish to maximize?\n",
    "\n",
    "The likelihood function we are trying to maximize is the following. \n",
    "$$P(y|X) = \\prod_{i=1}^{N}\\hat{y}_i^{y_i}[1-\\hat(y_i)]^{1-y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Since a logarithm is a monotonic function, maximizing the $f(x)$ is equivalent to maximizing $\\ln [f(x)]$. Express part (a) as a cost function of the model parameters, $C(\\mathbf{w})$, that is the negative of the logarithm of (a).\n",
    "\n",
    "The cost function is given by:\n",
    "$$C(w) = -\\sum_{i=1}^{N}\\frac{1}{N}(y_i log(\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Calculate the gradient of the cost function with respect to the model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. ${\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$.\n",
    "\n",
    "The gradient of the cost function is given by\n",
    "$\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[(y-\\hat{y}_i)x_0, (y-\\hat{y}_i))x_1, (y-\\hat{y}_i))x_2\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Write out the gradient descent update equation, assuming $\\eta$ represents the learning rate.\n",
    "\n",
    "Assuming $w_i$ is a vector of weights:\n",
    "$$w_{i+1} = w_i - \\eta \\cdot \\nabla_{\\mathbf{w}}C(\\mathbf{w_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** \n",
    "Load the data and scatter plot the data by class. In the data folder in the same directory of this notebook, you'll find the data in `A3_Q1_data.csv`. This file contains the binary class labels, $y$, and the features $x_1$ and $x_2$.  Comment on the data: do the data appear separable? Why might logistic regression be a good choice for these data or not?\n",
    "\n",
    "\n",
    "**The data does not appear to be perfectly separable. There are a number of points in class 1 contained within the class 2 cluster in the center. A logistic regression model would do well at classifying the class 0 points (in the center), but would have difficulty classifying the class 1 points contained in the center cluster. The code below loads the dataset and plots the groups by class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'A3_Q1_data.txt' does not exist: b'A3_Q1_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-2b99406d4089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A3_Q1_data.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'A3_Q1_data.txt' does not exist: b'A3_Q1_data.txt'"
     ]
    }
   ],
   "source": [
    "#Importing data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "data = pd.read_csv('A3_Q1_data.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting data\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from math import pow\n",
    "red_patch = mpatches.Patch(color='red', label='Class 0') #Generating legend colors and text\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class 1')\n",
    "cmap_class = ListedColormap(['#fc0320','#1f02fa' ]) #Creating corn-esque color map\n",
    "plt.scatter(data['x1'], data['x2'], c = data['y'], alpha = .5, cmap=cmap_class)\n",
    "plt.xlabel('X1 Value')\n",
    "plt.ylabel('X2 Value')\n",
    "plt.title('Distribution of Data by Class')\n",
    "plt.legend(handles = (red_patch, blue_patch))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** \n",
    "Do the data require any preprocessing due to missing values, scale differences, etc? If so, how did you remediate this?\n",
    "\n",
    "**There do not appear to be any missing observations in the data. The data looks pretty clean, as the classes are exactly evenly distributed and both x variables have simililar minimums, maximums, and standard deviations. It does not appear that any transformations are needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting summary statistics of data.\n",
    "print(data.describe())\n",
    "\n",
    "#Getting count of incomplete obervations. \n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Create a function or class to implement your logistic regression. It should take as inputs the model parameters, $\\mathbf{w}=\\left[w_0,w_1,w_2\\right]^{\\intercal}$, and output the class confidence probabilities, $P(Y=y_i|X=\\mathbf{x}_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining logistic regression class\n",
    "def logisticreg (x_values, weights):\n",
    "        line = np.exp(weights[0] + weights[1] * x_values[:,0] +  weights[2] * x_values[:,1]) \n",
    "\n",
    "        p = line/ (1+line)\n",
    "        return(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Create a function that computes the cost function $C(\\mathbf{w})$ for a given dataset and corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function calculator\n",
    "\n",
    "def CostFunction(y_value, predicted_prob):\n",
    "    cost = -np.average((y_value*np.log(predicted_prob+1e-7))+((-y_value+1)*np.log(- predicted_prob +1+1e-7)))\n",
    "    \n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** Create a function or class to run gradient descent on your training data. We'll refer to this as \"batch\" gradient descent since it takes into account the gradient based on all our data at each iteration (or \"epoch\") of the algorithm.  In doing this we'll need to make some assumptions about and/or experiment with the following:\n",
    "1. The initialization of the algorithm - what should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\n",
    "2. The learning rate - how slow/fast should the algorithm proceed in the direction opposite the gradient? This you will experiment with.\n",
    "3. Stopping criteria - when should the algorithm be finished searching for the optimum? Set this to be when the cost function changes by no more than $10^{-6}$ between iterations. Since we have a weight vector, you can compute this by seeing if the L2 norm of the weight vector changes by no more than $10^{-6}$ between iterations.\n",
    "\n",
    "Please compute your cost function for a batch as the average cost for the data in your current batch (in this case, a batch is your entire training dataset). In other words, divide your cost by the number of samples in each batch.\n",
    "\n",
    "**(j)** Design your approach so that at each step in the gradient descent algorithm it will produce updated parameter estimates. For each set of estimates, calculate the cost function for both the training and the test data (no output is necessary here, but this is used in the following question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "#Gradient descent algorithm that also returns costs. \n",
    "def GradientDescentCost(train_data, test_data, l_rate):\n",
    "   #Seperate x and y values\n",
    "    x_train = train_data[:,:-1]\n",
    "    y_train = train_data[:,-1]\n",
    "    x_test = test_data[:,:-1]\n",
    "    y_test = test_data[:,-1]\n",
    "    length = train_data.shape[0]\n",
    "    weights = np.array([random.random(), random.random(), random.random()])\n",
    "    train_costs = [] #Array to hold training and test costs for part k\n",
    "    test_costs = []\n",
    "    cost_train = CostFunction(y_train,logisticreg(x_train, weights)) #Find initial cost with random weights\n",
    "    cost_change = 10 #Create variable to hold cost change over each iteration. This will determine when the algorithm stops. \n",
    "    test_costs.append(CostFunction(y_test, logisticreg(x_test,weights))) #Add initial cost on test data.\n",
    "    train_costs.append(cost_train) #Add initial cost on training data. \n",
    "    while(cost_change > (10**-6)):\n",
    "        weights[0] = weights [0] - ((l_rate/length)* np.sum((logisticreg(x_train, weights) - y_train)))\n",
    "        weights[-2:]= weights[-2:] - ((l_rate / length) * (x_train.T @ (logisticreg(x_train, weights) - y_train)))\n",
    "        new_cost = CostFunction(y_train, logisticreg(x_train, weights))\n",
    "        cost_change = cost_train-new_cost\n",
    "        cost_train = new_cost\n",
    "        train_costs.append(cost_train)\n",
    "        test_costs.append(CostFunction(y_test, logisticreg(test_data,weights)))\n",
    "\n",
    "    return(weights, train_costs, test_costs)\n",
    "\n",
    "#Gradient descent algorithm that does not return costs. \n",
    "def GradientDescentFit(train_data, l_rate):\n",
    "   #Seperate x and y values\n",
    "    x_train = train_data[:,:-1]\n",
    "    y_train = train_data[:,-1]\n",
    "    length = train_data.shape[0]\n",
    "    weights = np.array([random.random(), random.random(), random.random()])\n",
    "    cost_train = CostFunction(y_train,logisticreg(x_train, weights)) #Find initial cost with random weights\n",
    "    cost_change = 10 #Create variable to hold cost change over each iteration. This will determine when the algorithm stops. \n",
    "    while(cost_change > (10**-6)):\n",
    "        weights[0] = weights [0] - ((l_rate/length)* np.sum((logisticreg(x_train, weights) - y_train)))\n",
    "        weights[-2:]= weights[-2:] - ((l_rate / length) * (x_train.T @ (logisticreg(x_train, weights) - y_train)))\n",
    "        new_cost = CostFunction(y_train, logisticreg(x_train, weights))\n",
    "        cost_change = cost_train-new_cost\n",
    "        cost_train = new_cost\n",
    "\n",
    "    return(weights)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(k)** Divide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent. Show the gradient descent process for different learning rates by plotting the resulting cost as a function of each iteration (or \"epoch\"). What is the impact that each parameter has on the process and the results? What choices did you make in your chosen approach and why? Use the parameter you choose here for the learning rate for the remainder of this question. \n",
    "\n",
    "**Larger learning rates generally make the cost for the logistic regression algorithm decrease faster. Your weights get updated 'more' when you are learning faster. This means that larger learning rates make your algorithm converage with less iterations. The problem is that you don't want to iterate too quickly and miss a perfect set of weights. I chose to go with a value of .1 for my learning rate, as I wanted a balance between iteration number and end cost.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training and test sets\n",
    "numpy_data = data.to_numpy()\n",
    "randnums = np.random.randn(len(data)) < .70\n",
    "training = numpy_data[randnums]\n",
    "test_data = numpy_data[~randnums]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to plot the cost trends.\n",
    "\n",
    "def costplot(GradOutput, l_rate):\n",
    "    plt.plot(GradOutput[1], label = 'Training Cost')\n",
    "    plt.title('Training and Test Cost With Learning Rate:{0:.5f}'.format(l_rate))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.plot(GradOutput[2], label = 'Test Cost')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Producing plots for values of the learning rate from .01 to .1, increasing by .01 each time.\n",
    "for i in range(10):\n",
    "    costplot(GradientDescentCost(training,test_data,(i/100)+.01),(i/100)+.010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(l)** Test the performance of your trained classifier using K-folds cross validation (while this can be done manually, the scikit-learn package [StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) may be helpful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4) # Spliting 4 times for cross validation \n",
    "X = numpy_data[:,:-1] #Separating values and y values\n",
    "y = numpy_data[:,-1]\n",
    "\n",
    "#Iterating to fit model on each fold\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    weights = GradientDescentFit(numpy_data[train], .10)\n",
    "    pred_probs = logisticreg(numpy_data[test],weights)\n",
    "    true_classes = y[test]\n",
    "    auc = metrics.roc_auc_score(true_classes,pred_probs)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_classes,  pred_probs)\n",
    "    plt.plot(fpr,tpr,label='Split:'+str(i+1)+', AUC={0:.5f}'.format(auc))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve For Model From Fold ' + str(i+1))\n",
    "    boundary = [0.0, 1.0]\n",
    "    plt.plot(boundary, boundary, label='Random Chance')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(m)** Why do we use cross validation?\n",
    "\n",
    "**When one fits a model, they generally fit it  on a training data set first. The goal of most modeling is to produce a model that will generalize well to any set of data that is provided. However, since we only fit our model on training data, we cannot be sure that our model will perform well in practice. Cross-validation allows one to gain some insight into how well a model will perform on unavailable data.\n",
    "Additionally, cross-validation also helps you determine how flexible your model should be. A model with high variance may have performance that changes wildly across each individual fitted fold. Checking cross-validation performance can help ensure that you aren't inadvertently overfitting. Thus, cross-validation can serve as one way you can compare potential models to choose from and select hyperparameters for your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(n)** Make two plots - one of your training data, and one for your test data - with the data scatter-plot and the decision boundary for your classifier. Comment on your decision boundary. Could it be improved?\n",
    "\n",
    "**It does appear that the decision boundary could be improved. Currently there are a number of observations within class 0 that are being misclassified. However, it looks like we would need to make our model more flexible in order to be able to capture the non-linearity of the decision boundary. This would require significant transformation of the data or a different classification technique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the weights for the model using the training data\n",
    "weights_train = GradientDescentFit(numpy_data, .10)\n",
    "predicted_train = logisticreg(numpy_data,weights_train)\n",
    "\n",
    "#Find the optimal cutoff value for the data. \n",
    "def Find_Cutoff(target, predicted):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold']) \n",
    "\n",
    "print(Find_Cutoff(y, predicted_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatterplot of both the training and test sets with the decision boundary displayed. \n",
    "from sklearn.datasets import make_moons \n",
    "import matplotlib\n",
    "from random import sample\n",
    "from matplotlib.colors import ListedColormap\n",
    "#Training data\n",
    "train_label = training[:,-1]\n",
    "#Calculate the weights for the model using the training data\n",
    "weights_train = GradientDescentFit(training, .80)\n",
    "\n",
    "\n",
    "\n",
    "#Creaing label/legend patches for graph\n",
    "red_patch = mpatches.Patch(color='red', label='Class 0') #Generating legend colors and text\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class 1')\n",
    "\n",
    "#Create grid of x values that we can apply our weights to. \n",
    "x1_min, x1_max = training[:, 0].min() - 1,training[:, 0].max() + 1\n",
    "x2_min, x2_max = training[:, 1].min() - 1, training[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n",
    "                     np.arange(x2_min, x2_max, 0.1))\n",
    "line = np.exp(weights_train[0] + weights_train[1] * xx +  weights_train[2] * yy) \n",
    "z =  line/ (1+line)\n",
    "Z = logisticreg((np.c_[xx.ravel(), yy.ravel()]), weights_train) #Generating predictions to be turned into colored regions of classes.\n",
    "Z = Z.reshape(xx.shape)\n",
    "Z = z.reshape(xx.shape)\n",
    "colors = ['red', 'blue']\n",
    "color_map = ListedColormap(['#FFAAAA', '#AAAAFF']) #Setting colors for background\n",
    "plt.scatter(training[:,0], training[:,1], c = train_label, s=20, cmap = matplotlib.colors.ListedColormap(colors))\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap = color_map)\n",
    "plt.title('Training Data With Decision Boundary')\n",
    "plt.xlabel('X1 Value')\n",
    "plt.ylabel('X2 Value')\n",
    "plt.legend(handles=[red_patch, blue_patch], loc = 'lower right', fontsize = '10')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Repeating the previous process using the test data\n",
    "test_label = test_data[:,-1]\n",
    "plt.scatter(test_data[:,0], test_data[:,1], c = test_label, s=20,  cmap = matplotlib.colors.ListedColormap(colors))\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap = color_map)\n",
    "plt.title('Test Data With Decision Boundary')\n",
    "plt.xlabel('X1 Value')\n",
    "plt.ylabel('X2 Value')\n",
    "plt.legend(handles=[red_patch, blue_patch], loc = 'lower right', fontsize = '10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Construct your dataset from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, which has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Your goal is to determine whether or not an example is a 3, therefore your binary classifier will seek to estimate $y=1$ if the digit is a 3, and $y=0$ otherwise. Create your dataset by transforming your labels into a binary format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Plot 10 examples of each class (i.e. class $y=0$, which are not 3's and class $y=1$ which are 3's), from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ten threes and 10 non-threes.\n",
    "y_train_arr = np.array(y_train)\n",
    "three_indices = np.where((y_train_arr==3))\n",
    "threes = X_train[three_indices]\n",
    "non_three_indices = np.where((y_train_arr!=3))\n",
    "non_threes = X_train[non_three_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing ten threes\n",
    "for i in range(10):\n",
    "    plt.imshow(threes[i])\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing non-threes\n",
    "figure = plt.figure()\n",
    "for i in [x for x in range(10) if x != 3]:\n",
    "    num_indices= np.where((y_train_arr==i))\n",
    "    num = X_train[num_indices]\n",
    "    if i <9:\n",
    "        plt.subplot(330 + i+1)\n",
    "        plt.imshow(num[0])\n",
    "    else:\n",
    "        plt.subplot(334)\n",
    "        plt.imshow(num[0])\n",
    "    #Removing labels\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "figure.suptitle('Examples of Non-Three Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** How many examples are present in each class? Show a histogram of samples by class. Are the classes balanced? What issues might this cause?\n",
    "\n",
    "**The classes are relatively balanced across each of the numbers. However, once we convert our target to being a binary 3 and non-three, we will have the vast majority as class non-three [approximately 89.78%]. It is possible that our algorithm will overpredict the other class in order to maximize/minimize whatever function it uses to fit itself.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how many occurances of each class. \n",
    "classes, counts = np.unique(y_train_arr, return_counts=True)\n",
    "class_count = dict(zip(classes, counts))\n",
    "print(class_count)\n",
    "#Plotting histogram of numbers/classes.\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "mybins =np.arange(y_train.min(), y_train.max()+2)\n",
    "ax1.hist(y_train, bins=mybins-.5,histtype='bar', ec='black')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('Number Written/Class')\n",
    "ax1.set_title('Frequency of Each Written Number')\n",
    "ax1.set_xticks(mybins)\n",
    "\n",
    "\n",
    "#Converting data from indiviudal numbers to 3 and  other.\n",
    "binary_train = (y_train == 3)\n",
    "binary_train = binary_train.astype(int)\n",
    "binary_test = (y_test == 3)\n",
    "binary_test = binary_test.astype(int)\n",
    "\n",
    "#Plotting histogram of numbers/classes after conversion\n",
    "mybins2 =np.arange(binary_train.min(), binary_train.max()+2)\n",
    "ax2.hist(binary_train, bins=mybins2-.5, histtype='bar', ec='black')\n",
    "ax2.set_title('Frequency of 3s and Non-3s')\n",
    "ax2.set_xlabel('Class 0 or 1: 1 Indicates 3s')\n",
    "ax2.set_ylabel('Count')\n",
    "plt.suptitle('Frequency of Written Classes Pre and Post-Processing')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Using cross-validation, train and test a classifier. Compare your performance against (1) a classifier that randomly guesses the class, and (2) a classifier that guesses that all examples are NOT 3's. Plot corresponding ROC curves and precision-recall curves. Describe the algorithm's performance and explain any discrepancies you find.\n",
    "\n",
    "**The random forest classifier I chose produced an overall accuracy of about 97%. Randomly guessing produced an accuracy of 50%, while predicting all not 3s produced an accuracy of 89.7%. If you examine the ROC and precision-recall curves it is clear the 'high-accuracy' all not 3s model performed very poorly. One should be careful about using accuracy as a metric, especially when classes are severely unbalanced.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "#Preprocessing images to be used by the classifier\n",
    "train_rows, train_numx, train_numy = X_train.shape\n",
    "train_dataset = X_train.reshape((train_rows,train_numx*train_numy))\n",
    "\n",
    "#Repeating for test data\n",
    "test_rows, test_numx, test_numy = X_test.shape\n",
    "test_dataset = X_test.reshape((test_rows,test_numx*test_numy))\n",
    "#K-fold cross validation. \n",
    "kfold4 = StratifiedKFold(n_splits = 4)\n",
    "for i, (train, validation) in enumerate(kfold4.split(train_dataset, binary_train)):\n",
    "    tree_small= tree.DecisionTreeClassifier()\n",
    "    tree_small.fit(train_dataset[train],binary_train[train])\n",
    "    small_tree_score = tree_small.score(train_dataset[validation], binary_train[validation])\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(train_dataset[train],binary_train[train])\n",
    "    forest_score = random_forest.score(train_dataset[validation],binary_train[validation])\n",
    "    print('Decision Tree:{}, Random Forest:{}'.format(small_tree_score, forest_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand\n",
    "#Creating random guess classifier\n",
    "def randomprobs(y_values):\n",
    "    guesses = np.random.rand(len(y_values),1)\n",
    "    return (guesses)\n",
    "\n",
    "\n",
    "#Creating vector where all are classified as not three [Class 0]\n",
    "\n",
    "def allones(y_values):\n",
    "    onevec = np.ones(len(y_values))\n",
    "    return(onevec-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classifier I will be using is the random forest classifier with default parameters. \n",
    "final_forest = RandomForestClassifier()\n",
    "final_forest.fit(train_dataset, binary_train)\n",
    "random_forest_predict = final_forest.predict_proba(test_dataset)[:,1]\n",
    "\n",
    "#Fitting Roc Curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(binary_test,  random_forest_predict)\n",
    "Rfpr, Rtpr, Rthresholds = metrics.roc_curve(binary_test,randomprobs(binary_test))\n",
    "plt.plot(fpr,tpr, label= 'Random Forest')\n",
    "Thrfpr, Thrtrp, thresholds = metrics.roc_curve(binary_test, allones(binary_test))\n",
    "plt.plot(Thrfpr, Thrtrp, label = 'All Non-3')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve For Random Forest Model')\n",
    "boundary = [0.0, 1.0]\n",
    "plt.plot(Rfpr, Rtpr, label='Random Chance')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "#Fitting Precision Recall Curve\n",
    "\n",
    "precision, recall,thresholds = precision_recall_curve(binary_test,  random_forest_predict)\n",
    "Rprecision, RRecall, Rthresholds = precision_recall_curve(binary_test,randomprobs(binary_test))\n",
    "ThrPrecision, ThRecall, thresholds = precision_recall_curve(binary_test, allones(binary_test))\n",
    "plt.plot(precision,recall, label= 'Random Forest')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision Recall Curve For Random Forest Model')\n",
    "plt.plot(ThrPrecision, ThRecall, label='Predicting All Non-3')\n",
    "plt.plot(Rprecision,RRecall, label='Random Prediction' )\n",
    "plt.legend(loc = 'center left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Using a logistic regression classifier (a linear classifier), apply lasso regularization and retrain the model and evaluate its performance over a range of values on the regularization coefficient. You can implement this using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module (DO NOT use your function from question 1) and activating the 'l1' penalty; the parameter $C$ is the inverse of the regularization strength. As you vary the regularization coefficient, plot (1) the number of model parameters that are estimated to be nonzero; (2) the logistic regression cost function, which you created a function to evaluate in the Question 1; (3) $F_1$-score, and (4) area under the curve (AUC). Describe the implications of your findings.\n",
    "\n",
    "**Increasing regularization decreased the number of non-zero parameters and the AUC values while increasing the cost associated with the model. The effect of increasing regularization on F1 score varied across the range of regularization values. One implication of this graph is that high amounts of regularization with lasso regression will push more and more coefficients to zero. This made the model slightly worse (in terms of AUC/cost) but the change was very small. Additionally, the F1 score actually improved once the regularization was strong enough. Another takeaway is that may not need to use all of the predictors you have in your model. You should do some form of feature selection especially if you have high-dimensional data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Building multiple linear regression models with lasso regularization Each one has a different regularization strength.\n",
    "num_params =[]\n",
    "model_costs = []\n",
    "f1_scores = []\n",
    "AUCs = []\n",
    "C_values = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(i)\n",
    "    LogModel = LogisticRegression(penalty='l1', C = 1/(i+1))\n",
    "\n",
    "    LogModel.fit(train_dataset, binary_train)\n",
    "    \n",
    "    #Generating probabilities to calculate cost\n",
    "    pred_logmodel = LogModel.predict_proba(train_dataset)[:,1]\n",
    "    \n",
    "    #Generating predicted classes to calculate F1 score\n",
    "    predicted_log_classes = LogModel.predict(train_dataset)\n",
    "    \n",
    "    f1_scores.append(metrics.f1_score(binary_train, predicted_log_classes))\n",
    "\n",
    "    num_params.append(np.count_nonzero(LogModel.coef_)) #Add number of non-zero parameters.\n",
    "\n",
    "    model_costs.append(CostFunction(binary_train, pred_logmodel)) #Add cost to array\n",
    "\n",
    "    AUCs.append(metrics.roc_auc_score(binary_train, pred_logmodel))\n",
    "    C_values.append(1/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3, ax4)) = plt.subplots(4,1, figsize=(10,20))\n",
    "#plotting auc\n",
    "ax1.plot(AUCs, label= 'AUC')\n",
    "ax1.set_title('AUC Values and Regularization Strength')\n",
    "ax1.set_ylabel('AUC Values')\n",
    "ax1.set_xlabel('Regularization Strength')\n",
    "\n",
    "#Plotting costs\n",
    "ax2.plot(model_costs)\n",
    "ax2.set_title('Cost and Regularization Strength')\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.set_xlabel('Regularization Strength')\n",
    "\n",
    "#Plotting parameter number\n",
    "ax3.plot(num_params)\n",
    "ax3.set_title('Number of Parameters and Regularization Strength')\n",
    "ax3.set_ylabel('Number of Parameters')\n",
    "ax3.set_xlabel('Regularization Strength')\n",
    "#plotting f1 scores\n",
    "ax4.plot(f1_scores)\n",
    "ax4.set_ylabel('F1 Score')\n",
    "ax4.set_xlabel('Regularization Strength')\n",
    "ax4.set_title('F1 Score and Regularization Strength')\n",
    "plt.suptitle('Trends in Model Metrics With Increasing Regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Plot the probability of each class conditional distribution (e.g. likelihood function), $P(x|C_0)$ and $P(x|C_1)$ on the sample plot in the domain $x \\in [0,2]$. *You can use [`scipy`'s `expon` module](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html#scipy.stats.expon) for this. Note that the `scale` parameter for this module is defined as $1/\\lambda$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon \n",
    "\n",
    "#Plotting distribution of x for class 0\n",
    "zero= expon(scale=1)\n",
    "one = expon(scale=.2)\n",
    "\n",
    "\n",
    "#Up to 2 in range for values\n",
    "dist_0 = np.linspace(0, np.minimum(zero.dist.b, 2)) \n",
    "dist_1 = np.linspace(0, np.minimum(one.dist.b, 2)) \n",
    "plt.plot(dist_0, zero.pdf(dist_0), label='Class 0 Distribution')\n",
    "plt.plot(dist_1, one.pdf(dist_1), label= 'Class 1 Distribution') \n",
    "plt.xlabel('X Value')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title('Class Conditional Distributions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "We are given that the distribution of x in each class is the following:\n",
    "$$p(x|c_0) = e^{-x}$$\n",
    "$$p(x|c_1) = 5e^{-5x}$$\n",
    "\n",
    "The conditional probability of x class 0 can be written as:\n",
    "$$p(c_0|x) = \\frac{p(x|c_0) \\cdot p(c_0)}{\\sum_{i=0}^{1} p(x|c_i) \\cdot p(c_i)}$$\n",
    "\n",
    "We want to find a value of x such that:\n",
    "$$p(c_0|x) >.50$$\n",
    "Thus:\n",
    "$$\\frac{p(x|c_0) \\cdot p(c_0)}{\\sum_{i=0}^{1} p(x|c_i) \\cdot p(c_i)} >.50$$\n",
    "\n",
    "Substituting our values we know into the equation gives:\n",
    "\n",
    "$$\\frac{{e^{-x}}}{(e^{-x} + 5e^{-5x})} >.50 $$\n",
    "\n",
    "Solving for x gives:\n",
    "$$ x > .402359$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating bayes decision rule classifier.\n",
    "\n",
    "def bayesclass(scale_0, scale_1, prob_class_0 ,x, cutoff):\n",
    "    zero_dist = expon(scale = scale_0) #Create first dist\n",
    "    one_dist = expon(scale = scale_1) #Create second dist\n",
    "    evidence = (zero_dist.pdf(x)*prob_class_0) + (one_dist.pdf(x)*(1-prob_class_0)) #Calc evidence\n",
    "    prob_0 = (zero_dist.pdf(x)*prob_class_0)/evidence\n",
    "    preds = prob_0>cutoff #Check if probability is greater than the cutoff\n",
    "    preds = ~preds\n",
    "    return (preds.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** \n",
    "**The value of x we solved for appears to correspond to the place on the graph where the two lines intersect. This makes sense, as the line for the class 0 likelihood is above the class 1 line for values greater than .402359. This means that it is more likely for the class to be of class 0 once you exceed that value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Load the test data in the file `A3_Q3_test.csv`. Apply your decision rule to the data. What is the misclassification rate (error rate, or fraction of misclassified samples) of this decision rule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "bayes_test_data = pd.read_csv('A3_Q3_test.txt')\n",
    "x_bayes_test = bayes_test_data.to_numpy()[:,1]\n",
    "\n",
    "#Making predictions with bayes classifer. \n",
    "bayes_labels = bayes_test_data.to_numpy()[:,-1]\n",
    "bayes_predictions =bayesclass(1,.2,.5,x_bayes_test,.402359)\n",
    "#Calculating overall accuracy. \n",
    "error_rate_bayes = 1 - np.mean(bayes_predictions==bayes_labels)\n",
    "\n",
    "print('The misclassification error of the Bayes Classifier on test data is {}.'.format(error_rate_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Load the training data in the file `A3_Q3_train.csv` and train a logistic regression classifier on the data (using default parameters). What is your misclassification error for your test dataset? How does this compare with the Bayes' classifier?\n",
    "\n",
    "\n",
    "**The misclassification error for the logistic regression model on the test dataset is nearly identical to the bayes classifier [.244 vs .233].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in training data\n",
    "train_bayes_data = pd.read_csv('A3_Q3_train.csv')\n",
    "train_x_bayes = train_bayes_data.to_numpy()[:,1]\n",
    "train_x_bayes = np.reshape(train_x_bayes,(-1,1))\n",
    "\n",
    "#Making predictions with bayes classifer. \n",
    "train_bayes_labels = train_bayes_data.to_numpy()[:,-1]\n",
    "\n",
    "#Fitting Logistic model\n",
    "\n",
    "logit_mod = LogisticRegression()\n",
    "logit_mod.fit(train_x_bayes, train_bayes_labels)\n",
    "\n",
    "#Checking accuracy on test data\n",
    "log_class_pred = logit_mod.predict(x_bayes_test.reshape(-1,1))\n",
    "error_rate_logistic = 1 - np.mean(log_class_pred==bayes_labels)\n",
    "\n",
    "print('The misclassification error of the Logistic Classifier on test data is {}.'.format(error_rate_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** What is your decision rule for your logistic regression model? To compute this, extract the parameters from your fit model (look for the `coef_` and `intercept_` attributes) and since the classes are balanced, the decision rule will be to classify a sample $x$ as Class 1 when your logistic regression sigmoid is greater than 0.5 (the halfway point from the two extremes of 0 and 1). How does this compare with the Bayes' classifier?\n",
    "\n",
    "**In order to get a value of the logistic regression sigmoid > .5 we need a certain x value. The intercept is -3.97529426 and the coefficient on x is 1.60336319. We can use these values to solve for the x that produces a probability of class 0 greater than .5. The process of solving this equation is detailed below.**\n",
    "\n",
    "$$ P(Class_0) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-(-3.97529426x+1.60336319)}}  $$ \n",
    "\n",
    "$$\\frac{\\mathrm{1} }{\\mathrm{1} + e^{-(-3.97529426x+1.60336319)}}  >.50$$ \n",
    "\n",
    "$$ x >.403072$$\n",
    "\n",
    "**It turns out that the logistic regression model produces a nearly identical decision rule to the Bayes classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing coefficients and intercepts.\n",
    "print(logit_mod.coef_)\n",
    "print(logit_mod.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** If the prior probabilities were not $P(C_0)=P(C_1)=0.5$, but instead if P(C_1)>P(C_0). How would this impact the optimal decision rule? Would it change, if so, would $x^*$ be larger or smaller?\n",
    "\n",
    "**Changing the prior probabilites would impact the optimal decision rule. If you had less of a certain class it would make it more unlikely that you would predict that class. If you increased the amount of instances of class 1 the boundary that x would have to exceed to yield class 0 would also increase. $x^*$ would become larger.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Identify a question or problem that's of interest to you and that could be addressed using classification or regression. Explain why it's interesting and what you'd like to accomplish. You're encouraged to be creative.\n",
    "\n",
    "**I plan to use lung image data to build a model that determines whether or not you have tuberculosis based on an x-ray of your lung. The goal would be to flag images as high or low priority to be reviewed by a physician. An algorithm of this type could lead to many benefits for those recieving healthcare and to the healthcare system as a whole. X-ray images are generally evaluated by a radiologist who, as a physician, has very valuable time. Reducing the amount of images a radiologist has to examine could lead to a cost reduction for patients and hospitals. Additionally, given how contagious and dangerous tuberculosis can be, being able to identify the presence of disease quickly may help prevent further infection or death. The dataset I used was obtained from the National Library of Medicine in collaboration with Shenzhen No.3 People's Hosptal and contains 632 lung images. Half of the images display tuberculosis infections and and half do not.**\n",
    "\n",
    "Citations:\n",
    "\n",
    "1)\tJaeger S, Karargyris A, Candemir S, Folio L, Siegelman J, Callaghan F, Xue Z, Palaniappan K, Singh RK, Antani S, Thoma G, Wang YX, Lu PX, McDonald CJ.  Automatic tuberculosis screening using chest radiographs. IEEE Trans Med Imaging. 2014 Feb;33(2):233-45. doi: 10.1109/TMI.2013.2284099. PMID: 24108713\n",
    "\n",
    "2)\tCandemir S, Jaeger S, Palaniappan K, Musco JP, Singh RK, Xue Z, Karargyris A, Antani S, Thoma G, McDonald CJ. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Trans Med Imaging. 2014 Feb;33(2):577-90. doi: 10.1109/TMI.2013.2290491. PMID: 24239990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Download the data and plot the data to describe it. You can use any dataset of interest to you with the exception of the Iris dataset, the Kaggle Titanic dataset, or the Kaggle chocolate dataset. Possible sources of dataset include (but are not limited to):\n",
    "\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- [Amazon Open Datasets](https://registry.opendata.aws/)\n",
    "- [Microsoft's Open Data](https://msropendata.com/)\n",
    "- [Google's Dataset Search](https://datasetsearch.research.google.com/)\n",
    "- [Awesomedata's list of datasets](https://github.com/awesomedata/awesome-public-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "from skimage.transform import resize\n",
    "from skimage import io\n",
    "import os \n",
    "#Specifying path to get data from \n",
    "\n",
    "path = r'C:/Users/Joe Krinke/Desktop/pulmonary-chest-xray-abnormalities/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png/'\n",
    "os.chdir(path)\n",
    "#Create array to hold data\n",
    "lung_images = []\n",
    "labels = []\n",
    "\n",
    "#Read in Images and resize them\n",
    "for i in range(326):\n",
    "    number = str(i+1)\n",
    "    number = number.rjust(4, '0')\n",
    "    name = str(r'CHNCXR_') + number + r'_0.png'  #Create filename, padding the number in the center to match the format.[CHNCXR_0001_0] Last 0 indicates no disease.  \n",
    "    unprocessed = io.imread(name,as_gray = True)\n",
    "    resized = resize(unprocessed, (1000,1000)) #Resize images to standard size\n",
    "    lung_images.append(resized)\n",
    "    labels.append(0) #Add label indicating they are negative for disease. \n",
    "#Positive images go from 327 - 662 inclusive\n",
    "for i in range(327,662):\n",
    "    number = str(i+1)\n",
    "    number = number.rjust(4, '0')\n",
    "    name = str(r'CHNCXR_') + number + r'_1.png'  #Create filename, padding the number in the center to match the format.[CHNCXR_0001_1] Last 1 indicates disease.  \n",
    "    unprocessed = io.imread(name, as_gray = True)\n",
    "    resized = resize(unprocessed, (1000,1000))\n",
    "    lung_images.append(resized)\n",
    "    labels.append(1) \n",
    "\n",
    "#Convert data\n",
    "lung_images = np.stack(lung_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing two sample classes to show the data. \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5)) \n",
    "healthy = io.imread('CHNCXR_0001_0.png', as_gray=True)\n",
    "ax1.imshow(healthy, cmap ='gray',vmin=0, vmax=255)#Printing healthy image\n",
    "ax1.set_title('Healthy Lungs')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "unhealthy = io.imread('CHNCXR_0555_1.png', as_gray = True)\n",
    "ax2.imshow(unhealthy,cmap ='gray', vmin=0, vmax=255) #Printing unhealthy image\n",
    "ax2.set_title('Unhealthy Lungs')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "plt.suptitle('Sample Images of Both Classes in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Formulate your supervised learning question: (a) What is your target variable (what are you trying to predict) and what predictors do you have available? Does your dataset require any preprocessing: is it clean (no missing values or erroneous data) and normalized (are each of the predictors of the same magnitude)? \n",
    "\n",
    "**My target variable is a binary class. 1 indicates the presence of pneumonia and 0 indicates healthy lungs. I have a set of 662 images of lungs (half healthy and half unhealthy) along with their corresponding labels. Each image is approximately 2900x2900, but the sizes can vary so I need to resize them to a standard size. Additionally, I need to convert the images from RGB to grayscale in order to reduce the dimensionality of the data. This transformation is especially appropriate since x-rays are black-and-white images to begin with. The data is clean and has been labeled by physicians. No normalization is necessary as the values of that pixel can take have the exact same range.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** What supervised learning technique will you use and why? \n",
    "\n",
    "**I plan to use logistic regression as my baseline model of performance. Logistic regression is simple and could work well if the data is linearly separable. The other model I plant to use is a random forest model. The random forest will be a better predictor if the classification boundary is more complex and non-linear.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Divide your dataset into training and testing datasets OR implement cross validation. Explain your approach and why you adopted it.\n",
    "\n",
    "**I chose to use a training and test dataset due to the large size of the image data. Each image, even after size reduction, is very complex. Fitting random forest models for each fold could quickly become time-consuming and computationally intensive. Consequently, I felt it was appropriate to evaluate whether my model overfit using the training and test data alone.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Divide data into training and test sets. \n",
    "xray_train, xray_test, xray_labels_train, xray_labels_test = train_test_split(lung_images, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Run your analysis and show your performance. Include plots of your data and of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Preprocessing images to be used by the classifier\n",
    "train_rows, train_numx, train_numy = xray_train.shape\n",
    "xray_train_fix = xray_train.reshape((train_rows,train_numx*train_numy))\n",
    "test_rows, test_numx, test_numy = xray_test.shape\n",
    "xray_test_fix = xray_test.reshape((test_rows, test_numx*test_numy))\n",
    "#Create logistic regression model as baseline. \n",
    "\n",
    "baseline = LogisticRegression()\n",
    "baseline.fit(xray_train_fix, xray_labels_train)\n",
    "xray_baseline_predict = baseline.predict_proba(xray_test_fix)[:,1]\n",
    "\n",
    "#Create random forest model. \n",
    "lung_forest = RandomForestClassifier()\n",
    "lung_forest.fit(xray_train_fix, xray_labels_train)\n",
    "xray_forest_predict = lung_forest.predict_proba(xray_test_fix)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curves for both models.\n",
    "BLfpr, BLtpr, BLthresholds = metrics.roc_curve(xray_labels_test,  xray_baseline_predict)\n",
    "RFfpr, RFtpr, RFthresholds = metrics.roc_curve(xray_labels_test,xray_forest_predict)\n",
    "plt.plot(RFfpr,RFtpr, label= 'Random Forest Model')\n",
    "plt.plot(BLfpr, BLtpr, label = 'Baseline Logistic Model')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve For Random Forest and Logistic Baseline')\n",
    "plt.plot(Rfpr, Rtpr, label='Random Chance')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "print(metrics.roc_auc_score(xray_labels_test,  xray_baseline_predict),metrics.roc_auc_score(xray_labels_test,  xray_forest_predict ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "#Evaluate confusion matrix on the test data\n",
    "confusion_matrix_forest = confusion_matrix(lung_forest.predict(xray_test_fix), xray_labels_test)\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix_forest, index = [i for i in ('Positive', 'Negative')],\n",
    "                  columns = [i for i in ('Predicted Positive', 'Predicted Negative') ])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title('Confusion Matrix for Random Forest Classifier')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "sn.heatmap(confusion_matrix_df, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Describe how your system performed, where your supervised learning algorithm performed well, where it did not, and how you could improve it. Summarize the conclusions from your work (this should involve a degree of interpretation more so than \"my classifier achieved an AUC of 0.8\").\n",
    "\n",
    "**The random forest model performed fairly well overall, with a test set accuracy of 78 % and AUC of .827. This performance isn't terrible given the classes were equally balanced. However, the baseline logistic regression model was able to achieve a higher AUC of .897. This is likely due to the fact that there is a lot of noise in the image data. Some noise factors were that images were resized from different original sizes, images were taken on patients who filled up the x-ray machine differently, and images were taken on people of different genders. Potential improvements to the modeling could include more complex image pre-processing. Images could be cropped to eliminate blank space, features could be generated from 'influential' areas (in the center of the lungs). or some kind of information about the gender of the patient could also be incorporated. Different models may have also been able to fit the data better. It is possible that the flexibility of the random forest ended up being its downfall- it may have overfit to the data. We could try techniques like PCA or logistic regression with regularization to reduce the amount of features.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "722px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
